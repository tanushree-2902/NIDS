# -*- coding: utf-8 -*-
"""Copy of UNSW_NB15 (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MIW8-jlhrRtI9KBmsfh3ZGEJ1x_SyO-T
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import os
sns.set_style('whitegrid')
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df1 = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW-NB15_1 (1).csv", header=None)

df_col = pd.read_csv('/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv', encoding='ISO-8859-1')

df_col['Name'] = df_col['Name'].apply(lambda x: x.strip().replace(' ', '').lower())

df1.columns = df_col['Name']
df1.rename(columns={'Name': ' '}, inplace=True)
new_df1 = df1.rename(columns={'feature_to_remove': ' '})

df2 = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW-NB15_2 (1).csv")
df3 = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW-NB15_3 (1).csv")
df4 = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW-NB15_4 (1).csv")

df_col2 = pd.read_csv('/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv', encoding='ISO-8859-1')

df_col2['Name'] = df_col2['Name'].apply(lambda x: x.strip().replace(' ', '').lower())

df2.columns = df_col2['Name']
df2.rename(columns={'Name': ' '}, inplace=True)
new_df2 = df2.rename(columns={'feature_to_remove': ' '})

df_col3 = pd.read_csv('/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv', encoding='ISO-8859-1')
df_col3['Name'] = df_col3['Name'].apply(lambda x: x.strip().replace(' ', '').lower())

df3.columns = df_col3['Name']
df3.rename(columns={'Name': ' '}, inplace=True)
new_df3 = df3.rename(columns={'feature_to_remove': ' '})

df_col4 = pd.read_csv('/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv', encoding='ISO-8859-1')

df_col4['Name'] = df_col4['Name'].apply(lambda x: x.strip().replace(' ', '').lower())

df4.columns = df_col4['Name']
df4.rename(columns={'Name': ' '}, inplace=True)
new_df4 = df4.rename(columns={'feature_to_remove': ' '})

#new_df4.head()

# new_df4.shape

features_df = pd.read_csv("/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv",encoding="latin-1")

features_df1 = features_df.drop(columns=['No.'])

# features_df1.head()

# Reading datasets
dfs = []
for i in range(1,5):
    path = '/content/drive/MyDrive/NIDS/UNSW-NB15_{} (1).csv'  # There are 4 input csv files
    dfs.append(pd.read_csv(path.format(i), header = None))
all_data = pd.concat(dfs).reset_index(drop=True)  # Concat all to a single df

# This csv file contains names of all the features
df_col = pd.read_csv('/content/drive/MyDrive/NIDS/NUSW-NB15_features (1).csv', encoding='ISO-8859-1')
# Making column names lower case, removing spaces
df_col['Name'] = df_col['Name'].apply(lambda x: x.strip().replace(' ', '').lower())
# Renaming our dataframe with proper column names
all_data.columns = df_col['Name']
all_data.rename(columns={'Name': ' '}, inplace=True)
new_all_data = all_data.rename(columns={'feature_to_remove': ' '})

# all_data.head()

# pd.set_option('display.max_columns', None)
# all_data.sample(15)

all_data.shape

#all_data.columns

# all_data.describe(include='O')

"""# data cleaning

"""

# all_data.isnull().sum()

# list(all_data.ct_flw_http_mthd.unique())

# all_data.state

# all_data['ct_flw_http_mthd'].value_counts()

# all_data[all_data.duplicated(keep='first')]

all_data[all_data.duplicated(keep='first')].shape #it selects all the duplicate rows in the data

all_data['state'] = all_data['state'].astype('category').cat.codes

all_data['proto'] = all_data['proto'].astype('category').cat.codes

all_data['service'] = all_data['service'].astype('category').cat.codes

all_data['attack_cat'] = all_data['attack_cat'].astype('category').cat.codes

all_data['ct_ftp_cmd'] = all_data['ct_ftp_cmd'].astype('category').cat.codes

all_data['srcip'] = all_data['srcip'].astype('category').cat.codes

all_data['dstip'] = all_data['dstip'].astype('category').cat.codes

all_data['sport'] = all_data['sport'].astype('category').cat.codes

#  all_data['sport'].nunique()

mean_value = all_data['is_ftp_login'].mean()
all_data['is_ftp_login'].fillna(mean_value, inplace=True)

mean_value = all_data['ct_flw_http_mthd'].mean()
all_data['ct_flw_http_mthd'].fillna(mean_value, inplace=True)

all_data['dsport'] = pd.to_numeric(all_data['dsport'], errors='coerce')

# Now you can drop the rows with NaN values
all_data = all_data.dropna(subset=['dsport'])

# Or you can fill them with a specific value, like 0
all_data['dsport'] = all_data['dsport'].fillna(0)

# And then convert the column to integer
all_data['dsport'] = all_data['dsport'].astype(int)

hexadecimal_string = '0xc0a8'
decimal_number = int(hexadecimal_string, 16)

all_data['dsport'] = all_data['dsport'].apply(lambda x: int(x, 16) if str(x).startswith('0x') else x)

all_data['dsport'] = all_data['dsport'].astype(str).astype(int)

all_data['dsport'] = all_data['dsport'].astype('category').cat.codes

# all_data.info()

# all_data.head()

"""# Data visualization"""

# attack_counts = all_data['attack_cat'].value_counts()
# names = list(attack_counts.index)
# bars = plt.bar(names, attack_counts.values,width=0.8)
# plt.bar(names, attack_counts.values)
# plt.gca().set_xticklabels(names)
# plt.xticks(rotation='vertical')

# plt.xlabel('Attack Category')
# plt.ylabel('Count')
# plt.title('Attack Category Distribution')


# for bar in bars:
#     yval = bar.get_height()
#     plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, round(yval, 2), ha='center', va='bottom')
# plt.show()

# all_data.nunique()

# all_data['state'].unique()

# all_data['state'].value_counts()

# state_counts = all_data['state'].value_counts()
# names = list(state_counts.index)
# bars = plt.bar(names, state_counts.values,width=0.8)
# plt.bar(names, state_counts.values)
# plt.gca().set_xticklabels(names)
# plt.xticks(rotation='vertical')

# plt.xlabel('State')
# plt.ylabel('Count')
# plt.title('State Category Distribution')


# for bar in bars:
#     yval = bar.get_height()
#     plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.005, round(yval, 2), ha='center', va='bottom')
# plt.show()

grouped_data = all_data.groupby(['state', 'label']).size().reset_index(name='counts')

# Create a bar plot with 'category' on the x-axis, 'counts' on the y-axis, and 'ip_group' as hue
bar_plot = sns.barplot(x='state', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
   width = p.get_width()
   height = p.get_height()
   x, y = p.get_xy() # Get the bottom left corner
   bar_plot.annotate(int(height), (x + width + 0.01, y + height * 1.02), ha='center',rotation=90)

plt.title('State Category Counts by Label')
plt.xlabel('State Category')
plt.ylabel('Counts')
plt.show()

all_data['service'].unique()

all_data['sttl'].unique()

all_data['dttl'].unique()

all_data['ct_ftp_cmd'].unique()

UNSW_NB15_testing = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW_NB15_testing-set (1).csv")

UNSW_NB15_training = pd.read_csv("/content/drive/MyDrive/NIDS/UNSW_NB15_training-set (1).csv")

UNSW_NB15_testing1 = UNSW_NB15_testing.drop(columns=['id'])

UNSW_NB15_testing1.rename(columns={'response_body_len': 'res_bdy_len', 'smean': 'smeansz', 'dmean': 'dmeansz', 'dinpkt': 'dintpkt','sinpkt': 'sintpkt'}, inplace=True)

print(all_data.columns)

all_data.shape

print(UNSW_NB15_testing1.columns)

# all_data.drop(['stime','ltime'], axis=1, inplace= True)

# UNSW_NB15_testing1.drop(['rate'], axis=1, inplace= True)

all_data.shape

UNSW_NB15_testing1.shape

UNSW_NB15_testing1['attack_cat'] = UNSW_NB15_testing1['attack_cat'].astype('category').cat.codes

UNSW_NB15_testing1['state'] = UNSW_NB15_testing1['state'].astype('category').cat.codes

UNSW_NB15_testing1['proto'] = UNSW_NB15_testing1['proto'].astype('category').cat.codes

UNSW_NB15_testing1['service'] = UNSW_NB15_testing1['service'].astype('category').cat.codes

# all_data.drop(['dstip','dsport','srcip','sport'], axis=1, inplace= True)

UNSW_NB15_training1 = UNSW_NB15_training.drop(columns=['id'])

g = all_data.groupby('attack_cat')
g

group_proto = all_data.groupby('proto')
group_proto

count_per_category = all_data.groupby('attack_cat').size()

# sorted_data_ascending = all_data.sort_values(by='attack_cat', ascending=True)
# print(sorted_data_ascending)

# sorted_data = all_data.sort_values("attack_cat")

# print(sorted_data.head(10))

selected_features = ['dur','sbytes','dbytes','sttl','dttl','sloss','dloss','sload','dload','spkts','dpkts','swin','dwin','stcpb','dtcpb','smeansz','dmeansz','trans_depth','res_bdy_len','sjit','djit','stime','ltime','sintpkt','dintpkt','tcprtt','synack','ackdat','is_sm_ips_ports','ct_state_ttl','ct_flw_http_mthd','is_ftp_login','ct_srv_src','ct_srv_dst','ct_dst_ltm','ct_src_ltm','ct_src_dport_ltm','ct_dst_sport_ltm','ct_dst_src_ltm','label']
correlation_matrix_selected = all_data[selected_features].corr()
print(correlation_matrix_selected)

sns.heatmap(correlation_matrix_selected, annot=True, fmt=".2f")
plt.show()

numeric_cols = all_data.select_dtypes(include=[np.number]).columns

# Compute the correlation matrix for numeric columns
correlation_matrix = all_data[numeric_cols].corr()

# Display the correlation matrix
print(correlation_matrix)

# Create a heatmap for better visualization
plt.figure(figsize=(10,  8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

correlation_matrix = all_data.corr()
print(correlation_matrix)

#highly corelated dpkts, dloss - 0.992163
 #                swin, din    - 0.997207
 #                dbytes, dpkts - 0.970808
 #               ct_dst-ltm, ct_src_dport_ltm - 0.960154
 #               sbytes, sloss - 0.953492

# dfCorr = all_data.drop(columns=['srcip', 'sport', 'dstip', 'dsport', 'proto', 'state', 'dur', 'sbytes',
#        'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', 'service', 'sload', 'dload',
#        'spkts', 'dpkts', 'swin', 'dwin', 'stcpb', 'dtcpb', 'smeansz',
#        'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit', 'djit', 'stime',
#        'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack', 'ackdat',
#        'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd', 'is_ftp_login',
#        'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm', 'ct_src_ltm',
#        'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm', 'attack_cat',
#        'label'])

# plt.figure(figsize=(10, 6))


# plt.hist('dpkts', bins=10, color='blue', alpha=0.7)
# plt.xlabel('Data Packets')
# plt.ylabel('Frequency')


# plt.hist('dloss', bins=10, color='red', alpha=0.7)
# plt.title('Relation between categories')
# plt.xlabel('Category')
# plt.show()

"""this specifies that dpkts and dloss are highly correlated

"""

all_data["label"].value_counts()

import ipaddress
from itertools import groupby

ip_addresses = ['59.166.0.0', '59.166.0.6', '59.166.0.5', '59.166.0.3',
       '10.40.182.3', '59.166.0.7', '10.40.170.2', '59.166.0.1',
       '59.166.0.2', '59.166.0.4', '175.45.176.3', '175.45.176.2',
       '175.45.176.0', '59.166.0.8', '59.166.0.9', '175.45.176.1',
       '10.40.182.1', '10.40.85.1', '192.168.241.243', '10.40.85.30',
       '149.171.126.16', '149.171.126.2', '149.171.126.11',
       '149.171.126.4', '149.171.126.5', '149.171.126.17',
       '149.171.126.19', '149.171.126.9', '149.171.126.8',
       '149.171.126.7', '149.171.126.15', '149.171.126.6',
       '149.171.126.0', '149.171.126.1', '149.171.126.3',
       '149.171.126.13', '149.171.126.12', '149.171.126.10',
       '149.171.126.18', '127.0.0.1', '149.171.126.14', '10.40.85.10',
       '10.40.182.6']

# Function to extract the first octet for grouping
def extract_first_octet(ip):
    return int(ipaddress.IPv4Address(ip).exploded.split('.')[0])

# Sort the IP addresses based on the first octet
sorted_ips = sorted(ip_addresses, key=extract_first_octet)

# Group the IP addresses by the first octet
grouped_ips = {key: list(group) for key, group in groupby(sorted_ips, key=extract_first_octet)}

# Print the result
for first_octet, ips in grouped_ips.items():
    print(f"Group {first_octet}: {ips}")

first_octets = []

# Iterate over the items in grouped_ips
for first_octet, ips in grouped_ips.items():
   # Append the first octet to the list
   first_octets.append(first_octet)

# Print the list of first octets
print(first_octets)

# Count the number of IP addresses in each group
grouped_ips_count = {key: len(ips) for key, ips in grouped_ips.items()}

# Print the result
for first_octet, count in grouped_ips_count.items():
   print(f"Group {first_octet}: {count} IP addresses")

print(grouped_ips_count)

# grouped = all_data.groupby('srcip')

# count = grouped.size().get('192.168.241.243')

# print(f'count: {count}')

# Function to extract the first octet for grouping
def extract_first_octet(ip):
   return int(str(ip).split('.')[0])

# Create a new column 'first_octet' in the DataFrame
all_data['first_octet'] = all_data['srcip'].apply(extract_first_octet)

mapping = {'0': 'Fuzzers', '1': 'Reconnaissance', '2': 'Shellcode', '3': 'Analysis', '4': 'Backdoor', '5': 'Backdoors', '6': 'DoS', '7': 'Exploits', '8': 'Generic', '9': 'Reconnaissance', '10': 'Shellcode', '11': 'Worms'}

# Rename the columns
all_data.rename(columns=mapping, inplace=True)

grouped_data = all_data.groupby(['attack_cat', 'label']).size().reset_index(name='counts')


bar_plot = sns.barplot(x='attack_cat', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
   width = p.get_width()
   height = p.get_height()
   x, y = p.get_xy() # Get the bottom left corner
   bar_plot.annotate(int(height), (x + width + 0.01, y + height * 1.02), ha='center',rotation=90)
   plt.xticks(rotation='vertical')

plt.title('Attack Category Counts by Label')
plt.xlabel('Attack Category')
plt.ylabel('Counts')
plt.show()

max_value = all_data['swin'].max()
min_value = all_data['swin'].min()

print(max_value)
print(min_value)

max_value = all_data['dwin'].max()
min_value = all_data['dwin'].min()

print(max_value)
print(min_value)

"""**SWIN**"""

bins = np.append(np.arange(0, 256, 20), 260) # Add 260 to the bin edges
labels = ['{}-{}'.format(i, j) for i, j in zip(bins[:-1], bins[1:])] # Adjust the labels to include 260

# Create a new column 'bins' in the DataFrame with the binned data
all_data['bins'] = pd.cut(all_data['swin'], bins=bins, labels=labels, right=False, include_lowest=True)

# Now perform the groupby operation
grouped = all_data.groupby('bins').size()

print(grouped)

# Define the bins and labels
bins = np.append(np.arange(0, 256, 20), 260) # Add 260 to the bin edges
labels = ['{}-{}'.format(i, j) for i, j in zip(bins[:-1], bins[1:])] # Adjust the labels to include 260

# Create a new column 'bins' in the DataFrame with the binned data
all_data['bins'] = pd.cut(all_data['swin'], bins=bins, labels=labels, right=False, include_lowest=True)

# Group data by 'bins' and count the number of IP groups in each bin
grouped_data = all_data.groupby('bins').size().reset_index(name='counts')

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis
bar_plot = sns.barplot(x='bins', y='counts', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('Grouped Counts by Bin')
plt.xlabel('Bin')
plt.ylabel('Counts')

plt.show()

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['bins', 'label']).size().reset_index(name='counts')

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='bins', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('Counts by Bin(swin) and Label')
plt.xlabel('Bin, Label')
plt.ylabel('Counts')

plt.show()

#33% of the network in the range of 0-20 swin value are getting attacked

"""
**DWIN**"""

bins = np.append(np.arange(0, 256, 20), 260) # Add 260 to the bin edges
labels = ['{}-{}'.format(i, j) for i, j in zip(bins[:-1], bins[1:])] # Adjust the labels to include 260

# Create a new column 'bins' in the DataFrame with the binned data
all_data['bins'] = pd.cut(all_data['dwin'], bins=bins, labels=labels, right=False, include_lowest=True)

# Now perform the groupby operation
grouped = all_data.groupby('bins').size()

print(grouped)

# Define the bins and labels
bins = np.append(np.arange(0, 256, 20), 260) # Add 260 to the bin edges
labels = ['{}-{}'.format(i, j) for i, j in zip(bins[:-1], bins[1:])] # Adjust the labels to include 260

# Create a new column 'bins' in the DataFrame with the binned data
all_data['bins'] = pd.cut(all_data['dwin'], bins=bins, labels=labels, right=False, include_lowest=True)

# Group data by 'bins' and count the number of IP groups in each bin
grouped_data = all_data.groupby('bins').size().reset_index(name='counts')

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis
bar_plot = sns.barplot(x='bins', y='counts', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('Grouped Counts by Bin')
plt.xlabel('Bin')
plt.ylabel('Counts')

plt.show()

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['sttl', 'label']).size().reset_index(name='counts')

plt.figure(figsize=(10, 6))

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='sttl', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('Counts by sttl and Label')
plt.xlabel('sttl, Label')
plt.ylabel('Counts')

plt.show()

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['dttl', 'label']).size().reset_index(name='counts')


plt.figure(figsize=(10, 6))

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='dttl', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center', va='center')
  plt.xticks(rotation='vertical')

plt.title('Counts by dttl and Label')
plt.xlabel('dttl, Label')
plt.ylabel('Counts')

plt.show()

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['bins', 'label']).size().reset_index(name='counts')

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='bins', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('Counts by Bin(Dwin) and Label')
plt.xlabel('Bin, Label')
plt.ylabel('Counts')

plt.show()

#33.5% of the network in the range of 0-20 of dwin value are getting attacked

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['service', 'label']).size().reset_index(name='counts')

plt.figure(figsize=(11, 6))
# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='service', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0), ha='center')
  plt.xticks(rotation='vertical')

plt.title('service and Label')
plt.xlabel('service, Label')
plt.ylabel('Counts')

plt.show()

# Group data by 'bins' and 'label', and count the number of records in each bin
grouped_data = all_data.groupby(['ct_ftp_cmd', 'label']).size().reset_index(name='counts')

plt.figure(figsize=(11, 6))

# Create a bar plot with 'bins' on the x-axis, 'counts' on the y-axis, and 'label' as hue
bar_plot = sns.barplot(x='ct_ftp_cmd', y='counts', hue='label', data=grouped_data, estimator=sum)

# Iterate through each bar and add the count number on top
for p in bar_plot.patches:
  width = p.get_width()
  height = p.get_height()
  x, y = p.get_xy() # Get the bottom left corner
  bar_plot.annotate(int(height), (x + width / 2, y + height * 1.0 + 0.5), ha='center', va='bottom')
  # plt.xticks(rotation='vertical')

plt.title('ct_ftp_cmd and Label')
plt.xlabel('ct_ftp_cmd, Label')
plt.ylabel('Counts')

plt.show()

sampled_df = df1.sample(5000)
print("sample executed")

numerical_cols = df1.select_dtypes(include=[np.number]).columns.tolist()
#this line sets all the numerical columns from the dataset to the new variable called numerical_cols

# print(numerical_cols)

num_numerical_cols = len(numerical_cols)

print(num_numerical_cols)

# plt.figure()
# sns.pairplot(df1, vars=['dur', 'sport'], markers=["o", "s"])
# selected_columns = df1.iloc[:, 1:3]
# df_array = df1.to_numpy()
# result = df_array[:, 1:3]
# #sns.pairplot(df1[:,1:3], markers=["o", "s"])
# plt.show()

max_value = df1['dur'].max()
min_value = df1['dur'].min()

print(max_value)
print(min_value)

# df1['sport'].unique()

max_value_sbytes = df1['sbytes'].max()
min_value_sbytes = df1['sbytes'].min()

print(max_value_sbytes)
print(min_value_sbytes)

max_value_dbytes = df1['dbytes'].max()
min_value_dbytes = df1['dbytes'].min()

print(max_value_dbytes)
print(min_value_dbytes)

sample_df = df1.sample(frac=0.1)

selected_features = ['sbytes', 'dbytes']

# sns.pairplot(sample_df[selected_features])
# plt.show()

numerical_data = all_data.select_dtypes(include=[np.number])

numerical_data.info()

print(numerical_data)

# numerical_data.count()

all_data['attack_cat'] = all_data['attack_cat'].fillna(all_data['attack_cat'].mode()[0])

#all_data['label'].value_counts().sum()

categorical_data = all_data.select_dtypes(exclude=[np.number])

print(categorical_data)

# all_data['bins'] = all_data['bins'].astype('category')

# all_data['bins'] = all_data['bins'].cat.codes

categorical_data.info()

categorical_data.count()

df1['ct_ftp_cmd'].unique()

all_data['ct_ftp_cmd'].unique()

all_data['ct_ftp_cmd'].value_counts()

df1['ct_ftp_cmd'].value_counts()

new_df4.info()

df1['ct_ftp_cmd'].value_counts().sum()

sample_data =  df1.sample(frac=1)

print(df1.columns)

print(sample_data.columns)

import seaborn as sns
import matplotlib.pyplot as plt
sns.pairplot(df1[['srcip','sport','dstip','dsport','proto','state','service','ct_ftp_cmd','attack_cat']])
plt.show()

all_data['attack_cat'].value_counts()

all_data['attack_cat'].value_counts().sum()

all_data.isnull().sum()

categorical_data.isnull().sum()

numerical_data.tail()

all_data.head()

categorical_data.tail()

categorical_data.value_counts()

all_data.isnull().sum()

important_features = ['state','swin', 'attack_category','sttl', 'dttl', 'service','ct_ftp_cmd','']

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
X = all_data.drop('label', axis=1)
y = all_data['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

sm = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)

print(pd.DataFrame(X_train).dtypes)

# Modelling
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint

# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz

rf = RandomForestClassifier()

from imblearn.over_sampling import SMOTE

# Apply SMOTE oversampling to the training data
sm = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)

# Train your model on the resampled data
rf.fit(X_train_resampled, y_train_resampled)

# training the model

rf.fit(X_train, y_train)

all_data['label'].value_counts()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# testing the model
y_pred = rf.predict(X_test)

print("Accuracy: ",accuracy_score(y_test, y_pred))
print("Precision: ",precision_score(y_test, y_pred))
print("Recall: ",recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))
print("roc_auc_score: ", roc_auc_score(y_test, y_pred))

from sklearn.metrics import accuracy_score
score = accuracy_score(y_test, y_pred)

print(score)

pred_train = rf.predict(X_train)
accuracy_score(y_train, pred_train)

cm = confusion_matrix(y_test, y_pred)

# Create a ConfusionMatrixDisplay instance
disp = ConfusionMatrixDisplay(confusion_matrix=cm)

disp.plot()

import pickle
pickle_out = open("classifier.pkl", mode = "wb")
pickle.dump(rf, pickle_out)
pickle_out.close()

import os
print(os.path.abspath("classifier.pkl"))

!pip install -q streamlit

!pip install requests

import requests

# Commented out IPython magic to ensure Python compatibility.
# %env NGROK_AUTHTOKEN=2bi1okFvyitBKPudZv7xwS4ayW0_2j5xGGqsYvaRJUeDcmpDx

authtoken = os.environ.get("NGROK_AUTHTOKEN")
if authtoken is None:
    raise ValueError("NGROK_AUTHTOKEN environment variable is not set.")

!streamlit version

!pip install localtunnel
!streamlit run app.py &>/dev/null&
!npx localtunnel --port 8501

!ngrok authtoken 2bi1okFvyitBKPudZv7xwS4ayW0_2j5xGGqsYvaRJUeDcmpDx

print(traceback.format_exc())

!netstat -an | grep 8501

.ngrok2/ngrok.yml

!pip install --upgrade pyngrok

from pyngrok import ngrok

# Start the Streamlit server
get_ipython().system_raw('streamlit run app.py &')

# Expose the Streamlit server to the internet using ngrok
public_url = ngrok.connect(port='80')
print(public_url)

!curl http://localhost:8501

print(traceback.format_exc())

!pip show ngrok

# !streamlit run app.py

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# 
# st.write('Hello, World!')

from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy: %.3f' % accuracy)

print("Accuracy: ",accuracy_score(y_test, y_pred))
print("Precision: ",precision_score(y_test, y_pred))
print("Recall: ",recall_score(y_test, y_pred))
print("F1 score: ", f1_score(y_test, y_pred))

from sklearn.model_selection import RandomizedSearchCV

param_dist = {
    'max_depth': [3, None],
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.5, 0.7, 1],
    'colsample_bytree': [0.5, 0.7, 1],
    'gamma': [0, 0.5, 1]
}

random_search = RandomizedSearchCV(XGBClassifier(), param_distributions=param_dist, n_iter=100, cv=2, verbose=2, random_state=42, n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
!pip install ipython-autotime
# %load_ext autotime

random_search.fit(X_train, y_train)

print("Best Parameters: ", random_search.best_params_)
print("Best Score: ", random_search.best_score_)

best_params_ = {
     'subsample': 0.5,
     'n_estimators': 300,
     'max_depth': 3,
     'learning_rate': 0.2,
     'gamma': 0.5,
     'colsample_bytree': 1
}

model1 = XGBClassifier(**best_params_)

model1.fit(X_train, y_train)

predictions = model1.predict(X_test)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(model1, X_train, y_train, cv=5)

print("Cross-Validation Accuracy Scores", scores)
print("Mean Cross-Validation Accuracy Score", np.mean(scores))

from sklearn.metrics import classification_report

print(classification_report(y_test, predictions))

from sklearn.linear_model import LogisticRegression

# model training
logreg = LogisticRegression()
logreg.fit(X_train, y_train)

# prediction making
y_pred = logreg.predict(X_test)

print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))

from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier()
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print('Accuracy:', metrics.accuracy_score(y_test, y_pred))

print("Number of samples in y_train: ", len(y_train))
print("Number of samples in y_pred: ", len(y_pred))

# train_accuracy = accuracy_score(y_train, y_pred)
# test_accuracy = accuracy_score(y_test, y_test_pred)

# print('Training Accuracy: ', train_accuracy)
# print('Testing Accuracy: ', test_accuracy)

from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier

from collections import Counter

def gini_index(groups, classes):
   n_instances = float(sum([len(group) for group in groups]))
   gini = 0.0
   for group in groups:
       size = float(len(group))
       if size == 0:
           continue
       score = 0.0
       for class_val in classes:
           p = [row[-1] for row in group].count(class_val) / size
           score += p * p
       gini += (1.0 - score) * (size / n_instances)
   return gini

def test_split(index, value, dataset):
   left, right = list(), list()
   for row in dataset:
       if row[index] < value:
           left.append(row)
       else:
           right.append(row)
   return left, right

def get_split(dataset):
   class_values = list(set(row[-1] for row in dataset))
   b_index, b_value, b_score, b_groups = 999, 999, 999, None
   for index in range(len(dataset[0])-1):
       for row in dataset:
           groups = test_split(index, row[index], dataset)
           gini = gini_index(groups, class_values)
           if gini < b_score:
               b_index, b_value, b_score, b_groups = index, row[index], gini, groups
   return {'index':b_index, 'value':b_value, 'groups':b_groups}

def to_terminal(group):
   outcomes = [row[-1] for row in group]
   return max(set(outcomes), key=outcomes.count)

def split(node, max_depth, min_size, depth):
   left, right = node['groups']
   del(node['groups'])
   if not left or not right:
       node['left'] = node['right'] = to_terminal(left + right)
       return
   if depth >= max_depth:
       node['left'], node['right'] = to_terminal(left), to_terminal(right)
       return
   if len(left) <= min_size:
       node['left'] = to_terminal(left)
   else:
       node['left'] = get_split(left)
       split(node['left'], max_depth, min_size, depth+1)
   if len(right) <= min_size:
       node['right'] = to_terminal(right)
   else:
       node['right'] = get_split(right)
       split(node['right'], max_depth, min_size, depth+1)

def build_tree(train, max_depth, min_size):
   root = get_split(train)
   split(root, max_depth, min_size, 1)
   return root

def predict(node, row):
   if row[node['index']] < node['value']:
       if isinstance(node['left'], dict):
           return predict(node['left'], row)
       else:
           return node['left']
   else:
       if isinstance(node['right'], dict):
           return predict(node['right'], row)
       else:
           return node['right']

def predict_model(model, test):
   predictions = []
   for row in test:
       prediction = predict(model, row)
       predictions.append(prediction)
   return predictions

# all_data.info()

# Load the dataset
# data = pd.read_csv('UNSW_NB15_training-set.csv')

# Preprocess the data
labelencoder = LabelEncoder()
sample_df['label'] = labelencoder.fit_transform(sample_df['label'])

# Split the dataset into features and target variable
X = sample_df.drop('label', axis=1)
y = sample_df['label']

# Convert the dataframe to a list of lists
X = X.values.tolist()
y = y.values.tolist()

# Split the dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a decision tree and fit it to the training data
tree = build_tree(X_train, max_depth=2, min_size=1)

# Make predictions on the test set
y_pred = predict_model(tree, X_test)

# Print the accuracy of the classifier
correct = sum([y_i==y_j for y_i, y_j in zip(y_pred, y_test)])
total = len(y_test)
accuracy = correct / total
print("Accuracy:", accuracy)

plt.rcParams.update({'font.size':16})
df1.hist(figsize=(60,40))

y_train_flattened = np.ravel(y_train)

# Print the flattened array
print(y_train_flattened)

X_train = np.array(X_train)
X_train = X_train.reshape(-1)

from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
le = LabelEncoder()

# Fit the LabelEncoder to the data in X_train
le.fit(X_train)

# Transform the data in X_train to numeric values
X_train_encoded = le.transform(X_train)

# Print the transformed data
print(X_train_encoded)

X_train = X_train.reshape(-1, 1)

all_data.head()

all_data.info()

le = LabelEncoder()
all_data['ip_first_octet'] = le.fit_transform(all_data['bins'])

all_data.drop('srcip', axis=1)
all_data.drop('dstip', axis=1)

def gini_impurity(y):
 if isinstance(y, pd.Series):
   p = y.value_counts()/y.shape[0]
   gini = 1-np.sum(p**2)
   return(gini)
 else:
   raise('Object must be a Pandas Series.')

# Call the function with a pandas series
gini = gini_impurity(all_data['attack_cat'])

print(f'Gini Impurity for attack_cat : {gini}')

def gini_impurity(y):
 if isinstance(y, pd.Series):
   p = y.value_counts()/y.shape[0]
   gini = 1-np.sum(p**2)
   return(gini)
 else:
   raise('Object must be a Pandas Series.')

for col in all_data.columns:
   # Calculate Gini impurity for the current column
   gini = gini_impurity(all_data[col])

   print(f'Gini Impurity for {col} : {gini}')

"""the gini impurity of **res_bdy_len** is minimum. hence this is taken as the the splitting feature for the decision tree.

"""

max_value_res_bdy_len = all_data['res_bdy_len'].max()
min_value_res_bdy_len = all_data['res_bdy_len'].min()

print(max_value_res_bdy_len)
print(min_value_res_bdy_len)

all_data["res_bdy_len"].nunique

all_data["res_bdy_len"].value_counts()

# function to calculate Gini impurity
def gini_impurity(y):
   p = y.value_counts() / len(y)
   gini = 1 - np.sum(p**2)
   return gini

# Define the function to calculate Gini impurity of a specific cut
def gini_impurity_cut(X, y, feature, threshold):
   mask = X[feature] <= threshold
   subset1 = y[mask]
   subset2 = y[~mask]
   p1 = subset1.value_counts() / len(subset1)
   p2 = subset2.value_counts() / len(subset2)
   gini1 = 1 - np.sum(p1**2)
   gini2 = 1 - np.sum(p2**2)
   gini_cut = (len(subset1) * gini1 + len(subset2) * gini2) / len(y)
   return gini_cut

# Calculate Gini impurity of the entire dataset
gini_total = gini_impurity(all_data['label'])
print(f'Total Gini Impurity: {gini_total}')

# Calculate Gini impurity of the cut at 3000000
gini_cut = gini_impurity_cut(all_data, all_data['label'], 'res_bdy_len',3000000)
#since the maximum value of data is 65 lakh so i took the middle value.
print(f'Gini Impurity of cut at res_bdy_len 3000000 : {gini_cut}')

def entropy(y):
   if(len(y) == 1):
       return 0
   else:
       p = y.value_counts()/len(y)
       entropy = np.sum([-p[i]*np.log2(p[i]) for i in p.index])
       return entropy

def variance(y):
  '''
  Function to help calculate the variance avoiding nan.
  y: variable to calculate variance to. It should be a Pandas Series.
  '''
  if(len(y) == 1):
    return 0
  else:
    return y.var()

def information_gain(y, mask, func=entropy):
  '''
  It returns the Information Gain of a variable given a loss function.
  y: target variable.
  mask: split choice.
  func: function to be used to calculate Information Gain in case os classification.
  '''

  a = sum(mask)
  b = mask.shape[0] - a

  if(a == 0 or b ==0):
    ig = 0

  else:
    if y.dtypes != 'O':
      ig = variance(y) - (a/(a+b)* variance(y[mask])) - (b/(a+b)*variance(y[-mask]))
    else:
      ig = func(y)-a/(a+b)*func(y[mask])-b/(a+b)*func(y[-mask])

  return ig

mask = all_data['res_bdy_len'] == '3924'
ig = information_gain(all_data['label'], mask)
print(f'Information Gain for cut where res_bdy_len = 3924: {ig}')

import pandas as pd

def categorical_options(a):
 '''
 Creates all possible combinations from a Pandas Series.
 a: Pandas Series from where to get all possible combinations.
 '''
 a = a.unique()
 return a.value_counts().index.tolist()

def max_information_gain_split(x, y, func=entropy):
 '''
 Given a predictor & target variable, returns the best split, the error and the type of variable based on a selected cost function.
 x: predictor variable as Pandas Series.
 y: target variable as Pandas Series.
 func: function to be used to calculate the best split.
 '''

 split_value = []
 ig = []

 try:
    x = pd.to_numeric(x)
    numeric_variable = True
 except ValueError:
    numeric_variable = False

 # Create options according to variable type
 if numeric_variable:
    options = x.sort_values().unique()[1:]
 else:
    options = categorical_options(x)

 # Calculate ig for all values
 for val in options:
    mask =   x < val if numeric_variable else x.isin(val)
    val_ig = information_gain(y, mask, func)
    # Append results
    ig.append(val_ig)
    split_value.append(val)

 # Check if there are more than 1 results if not, return False
 if len(ig) == 0:
    return(None,None,None, False)

 else:
 # Get results with highest IG
    best_ig = max(ig)
    best_ig_index = ig.index(best_ig)
    best_split = split_value[best_ig_index]
    return(best_ig,best_split,numeric_variable, True)

weight_ig, weight_split, _, _ = max_information_gain_split(all_data['res_bdy_len'],all_data['label'])

print(
 "The best split for res_bdy_len is when the variable is less than ",
 weight_split,"\nInformation Gain for that split is:", weight_ig
)

train_data = pd.read_csv('.csv')
X_train = train_data.drop('target', axis=1)
y_train = train_data['target']